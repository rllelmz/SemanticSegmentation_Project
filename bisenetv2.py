# -*- coding: utf-8 -*-
"""BiseNetV2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12h8N_ELWGJZ-41iftcssrHWt-f8Akt7O
"""

import keras 
from keras.models import Sequential
from keras.layers import BatchNormalization,Input, Activation, Dropout, Flatten, Conv2D, MaxPooling2D,AveragePooling2D,Concatenate,UpSampling2D,Add,Dropout
import tensorflow as tf
import numpy as np


backbone_url = 'https://github.com/CoinCheung/BiSeNet/releases/download/0.0.0/backbone_v2.pth'


#NB Filters is the nb of out_channels
#in_shape is the shape of the input item
def ConvBNReLU (in_, filters=64, ks=3, strides=1, padding="same",dilation=1, groups=1, bias=False) :
    activation ="relu"
    conv_ =  Conv2D(filters=filters,kernel_size = ks,strides=strides, padding=padding, dilation_rate=dilation,groups=groups, use_bias=bias)
    batch_norm = BatchNormalization()
    activation = Activation(activation)
    
    x = conv_(in_) 
    x = batch_norm(x)
    x = activation(x)
    return x

def DetailBranch(in_) :
        
        x=ConvBNReLU(in_,filters=64, ks=3, strides=2)
        x=ConvBNReLU(x,filters=64, ks=3, strides=1)
        x=ConvBNReLU(x,filters=64, ks=3, strides=2)
        
        x=ConvBNReLU(x,filters=64, ks=3, strides=1)
        x=ConvBNReLU(x,filters=64, ks=3, strides=1)
        x=ConvBNReLU(x,filters=128, ks=3, strides=2)
        x=ConvBNReLU(x,filters=128, ks=3, strides=1)
        x=ConvBNReLU(x,filters=128, ks=3, strides=1)
     
        return x

#left et right représente les deux méthodes de downsampling du stemblock, feat represente la concaténation des deux méthodes
def StemBlock(input):
        left = ConvBNReLU(input,16, 3, strides=2)
        left = ConvBNReLU(left, 8, strides=1, padding="valid")
        left= ConvBNReLU(left, 16, 3, strides=2)
        right= MaxPooling2D(pool_size=3, strides=1, padding="same")(left)

        feat = Concatenate(axis=1)([left, right])
        fuse = ConvBNReLU(feat,16, 3, strides=1)
        return feat

def CEBlock(in_):
        feat = np.mean(in_,axis=(2,3),keepdims=True)
        bn = BatchNormalization()
        feat=bn(feat)
        conv_gap = ConvBNReLU(feat, 128, 1, strides=1, padding="valid")
        feat= feat+ in_
        #TODO: in paper here is naive conv2d, no bn-relu
        conv_last = ConvBNReLU(feat, 128, 3, strides=1)

        return feat

def GELayerS1(in_,in_chan, out_chan, exp_ratio=6):

        mid_chan = in_chan * exp_ratio

        feat = ConvBNReLU(in_, in_chan, 3, strides=1)
        dwconv = ConvBNReLU(feat, mid_chan, ks=3, strides=1,padding="same", groups=in_chan, bias=False)
      
        conv2 = Conv2D(out_chan, kernel_size=1, strides=1,padding="valid", use_bias=False)(dwconv)
        feat =BatchNormalization()
        feat= feat(conv2)
        feat = feat+ in_
        activation = Activation("relu")
        feat =activation(feat)

        return feat
def GELayerS2(in_,in_chan, out_chan, exp_ratio=6):

  mid_chan = in_chan * exp_ratio

  feat = ConvBNReLU(in_, in_chan, 3, strides=1)
  dwconv1 = Conv2D(mid_chan, kernel_size=3, strides=2,padding="same", use_bias=False)(feat)
  feat =BatchNormalization()
  feat= feat(dwconv1)
        
  dwconv2 = ConvBNReLU(feat, mid_chan, ks=3, strides=1,padding="same", groups=mid_chan, bias=False)
            
  conv2 = Conv2D(out_chan, kernel_size=1, strides=1,padding="valid", use_bias=False)(feat)
  bn = BatchNormalization()
  feat=bn(conv2)
      
  shortcut = Conv2D(in_chan, kernel_size=3, strides=2,padding="same", groups=in_chan, use_bias=False)(in_)
  bn= BatchNormalization()
  shortcut=bn(shortcut)
  shortcut= Conv2D  (out_chan, kernel_size=1, strides=1,padding="valid", use_bias=False)(shortcut)
  bn= BatchNormalization()
  shortcut=bn(shortcut)
  feat=feat+shortcut
  activation = Activation("relu")
  feat =activation(feat)
  return feat
        

def SegmentBranch(in_):
      feat2 = StemBlock(in_)
      feat3 = GELayerS2(feat2,16, 32)
      feat3=  GELayerS1(feat3,32, 32)

      feat4 = GELayerS2(feat3,32, 64)
      feat4= GELayerS2(feat4,64, 64)
      feat5_4= GELayerS2(feat4,64, 128)
      feat5_4= GELayerS1(feat5_4,128, 128)
      feat5_4= GELayerS1(feat5_4,128, 128)
      feat5_4= GELayerS1(feat5_4,128, 128)
      
      feat5_5 = CEBlock(feat5_4)
      return feat2, feat3, feat4, feat5_4, feat5_5

def BGALayer(x_d,x_s):

        left1 = Conv2D(128, kernel_size=3, strides=1,padding="same", groups=128, use_bias=False)(x_d)
        bn=BatchNormalization()(left1)
        left1=Conv2D(128, kernel_size=1, strides=1,padding="valid", use_bias=False)(bn)

        left2 = Conv2D(128, kernel_size=3, strides=2,padding="same", use_bias=False)(x_d)
        bn= BatchNormalization()(left2)
        left2=AveragePooling2D(pool_size=3, strides=2, padding="same")(bn)
        
        right1 = Conv2D(128, kernel_size=3, strides=1,padding="same", use_bias=False)(x_s)
        bn= BatchNormalization()
        right1= bn(x_s)

        right2 = Conv2D(128, kernel_size=3, strides=1,padding="same", groups=128, use_bias=False)(x_s)
        bn= BatchNormalization()(right2)
        right2= Conv2D(128, kernel_size=1, strides=1,padding="valid", use_bias=False)(bn)

        up1 = UpSampling2D(size=(4, 4))
        right1=up1(right1)

        up2 = UpSampling2D(size=(4, 4))
      
        sig = Activation("sigmoid")
        

        left = left1 * sig(right1)
        right = left2 * sig(right2)
        right = up2(right)
        s=Add()([left, right])
        
        conv = Conv2D(128, kernel_size=3, strides=1,padding="same", use_bias=False)(s)
        bn= BatchNormalization()
        bn=bn(conv)
        activation = Activation("relu") # not shown in paper
        out=activation(bn)
        
        return out

def SegmentHead(in_, mid_chan, n_classes, up_factor=8, aux=True):
        conv = ConvBNReLU(in_,mid_chan, 3, strides=1)
        drop = Dropout(0.1)(conv)

       
        mid_chan2 = up_factor * up_factor if aux else mid_chan
        up_factor = up_factor // 2 if aux else up_factor
        conv_out=drop
        if aux :
          conv_out = UpSampling2D(size=(2, 2))(conv_out)
          conv_out = ConvBNReLU(conv_out,mid_chan2, 3, strides=1)
        conv_out=Conv2D(n_classes, 1, 1, "valid", use_bias=True)(conv_out)
        conv_out=UpSampling2D(size=(up_factor,up_factor), interpolation='bilinear')(conv_out)
        return conv_out

# def BiSeNetV2(in_,n_classes, aux_mode='train'):

#         detail = DetailBranch(in_)
#         feat2, feat3, feat4, feat5_4, feat_s = SegmentBranch(in_)
#         bga = BGALayer(detail,feat_s)
#         mode = aux_mode

#         head = SegmentHead(bga, 1024, n_classes, up_factor=8, aux=False)
#         if mode == 'train':
#           aux2 = SegmentHead(feat2, 128, n_classes, up_factor=4)
#           aux3 = SegmentHead(feat3, 128, n_classes, up_factor=8)
#           aux4 = SegmentHead(feat4, 128, n_classes, up_factor=16)
#           aux5_4 = SegmentHead(feat5_4, 128, n_classes, up_factor=32)
#           return head, aux2, aux3, aux4, aux5_4

#         elif mode == 'eval':
#             return  head

#         elif mode == 'pred':
#             pred = tf.math.argmax(head,axis=1)
#             return pred

#         else:
#             raise NotImplementedError

#        


######################################Partie Execution#################################################################################
#L'exécution se fait avec des tensors 

x= tf.random.normal([16, 1024, 2048, 3], mean=-1, stddev=4)
detail = DetailBranch(x)
print('detail', detail)

x= tf.random.normal([16, 1024, 2048, 3], mean=-1, stddev=4)   
stem = StemBlock(x)
print('stem', stem)

x= tf.random.normal([16, 16, 32, 128], mean=-1, stddev=4)   
ceb = CEBlock(x)
print('ceb',ceb)

x= tf.random.normal([16, 16, 32, 32], mean=-1, stddev=4)
ge1 = GELayerS1(x,32, 32)
print('ge1',ge1)

x= tf.random.normal([16, 16, 32, 16], mean=-1, stddev=4)    
ge2 = GELayerS2(x,16, 32)
print('ge2',ge2)

left = tf.random.normal([16,64, 128, 128], mean=-1, stddev=4)
right = tf.random.normal([16, 16, 32, 128], mean=-1, stddev=4)
bga = BGALayer(left, right)
print('bga',bga)

x= tf.random.normal([16, 1024, 2048, 3], mean=-1, stddev=4)   
segment = SegmentBranch(x)
print('SegmentBranch',segment)

x= tf.random.normal([16, 64, 128, 128], mean=-1, stddev=4)   
head = SegmentHead(x,128, 128, 19)
print('SegmentHead',head)